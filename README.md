

  <img src="./fig/bg.png" width="800" class="center">


 

# Typology of Prompt-based Learning

* ### Pre-trained Models
  * Left-to-right Language Model: [\[1\]](); [\[2\]]()
  * Masked Language Model: [\[1\]](); [\[2\]]()
  * Prefix Language Model: [\[1\]](); [\[2\]]()
  * Encoder-Decoder: [\[1\]](); [\[2\]]()
* ### Prompt Engineering
  * ##### Shape
    * Cloze prompt: [\[1\]](); [\[2\]]()
    * Prefix prompt: [\[1\]](); [\[2\]]()
  * ##### Human Effort
    * Hand-crated
    * Automated
        - Discrete: [\[1\]](); [\[2\]]()
        - Continuous: [\[1\]](); [\[2\]]()
* ### Answer Engineering
  * Shape
    * Token: [\[1\]](); [\[2\]]()
    * Span: [\[1\]](); [\[2\]]()
    * Sentence: [\[1\]](); [\[2\]]()
  * Human Effort
    * Hand-crated: [\[1\]](); [\[2\]]()
    * Automated
        - Discrete: [\[1\]](); [\[2\]]()
        - Continuous: [\[1\]](); [\[2\]]()
    
* ### Multi-Prompt Learning
  * Prompt Ensemble: [\[1\]](); [\[2\]]()
  * Prompt Augmentation: [\[1\]](); [\[2\]]()
  * Prompt Composition: [\[1\]](); [\[2\]]()
  * Prompt Decomposition: [\[1\]](); [\[2\]]()
  * Prompt Sharing: [\[1\]](); [\[2\]]()
    
* ### Prompt-based Training Strategies
  * Parameter Updating
    * Promptless Fine-tuning: [\[1\]](); [\[2\]]()
    * Tuning-free Prompting: [\[1\]](); [\[2\]]()
    * Fixed-LM Prompt Tuning: [\[1\]](); [\[2\]]()
    * Fixed-prompt LM Tuning: [\[1\]](); [\[2\]]()
    * Prompt+LM Tuning: [\[1\]](); [\[2\]]()
  * Training Sample Size
    * Zero-shot: [\[1\]](); [\[2\]]()
    * Few-shot: [\[1\]](); [\[2\]]()
    * Full-data: [\[1\]](); [\[2\]]()
    
 