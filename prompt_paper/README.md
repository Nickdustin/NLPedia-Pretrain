










FLEX: Unifying Evaluation for Few-Shot NLP15 July, 2021
Fixed-prompt LM Tuning

HTLM: Hyper-Text Pre-Training and Prompting of Language Models14 July, 2021
Fixed-prompt LM Tuning

Evaluating Large Language Models Trained on Code7 July, 2021
Tuning-free Prompting; Fixed-prompt LM Tuning

ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation5 July, 2021
Tuning-free Prompting

Multimodal Few-Shot Learning with Frozen Language Models25 June, 2021
Fixed-LM Prompt Tuning

Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models24 June, 2021
LM+Prompt Tuning

BARTScore: Evaluating Generated Text as Text Generation21 June, 2021
Tuning-free Prompting

CPM-2: Large-scale Cost-effective Pre-trained Language Models20 June, 2021
Fixed-LM Prompt Tuning; LM+Prompt Tuning

Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning16 June, 2021
Fixed-LM Prompt Tuning

Reordering Examples Helps during Priming-based Few-Shot Learning3 June, 2021
Tuning-free Prompting

Template-Based Named Entity Recognition Using BART3 June, 2021
Fixed-prompt LM Tuning

PTR: Prompt Tuning with Rules for Text Classification24 May, 2021
LM+Prompt Tuning

True Few-Shot Learning with Language Models24 May, 2021
Tuning-free Prompting; Fixed-prompt LM Tuning

PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation26 April, 2021
Tuning-free Prompting

Constrained Language Models Yield Few-Shot Semantic Parsers18 April, 2021
Tuning-free Prompting

Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity18 April, 2021
Tuning-free Prompting

Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions18 April, 2021
Tuning-free Prompting; Fixed-prompt LM Tuning

The Power of Scale for Parameter-Efficient Prompt Tuning18 April, 2021
Fixed-LM Prompt Tuning

Surface Form Competition: Why the Highest Probability Answer Isn't Always Right16 April, 2021
Tuning-free Prompting

AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction15 April, 2021
Fixed-prompt LM Tuning

Generating Datasets with Pretrained Language Models15 April, 2021
Tuning-free Prompting

Learning How to Ask: Querying LMs with Mixtures of Soft Prompts14 April, 2021
Fixed-LM Prompt Tuning

Factual Probing Is [MASK]: Learning vs. Learning to Recall12 April, 2021
Fixed-LM Prompt Tuning

Meta-tuning Language Models to Answer Prompts Better10 April, 2021
Fixed-prompt LM Tuning

Improving and Simplifying Pattern Exploiting Training22 March, 2021
Fixed-prompt LM Tuning

All NLP Tasks Are Generation Tasks: A General Pretraining Framework18 March, 2021
Fixed-prompt LM Tuning

GPT Understands, Too18 March, 2021
Tuning-free Prompting; LM+Prompt Tuning

How Many Data Points is a Prompt Worth?15 March, 2021
Fixed-prompt LM Tuning

BERTese: Learning to Speak to BERT9 March, 2021
Tuning-free Prompting

Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP27 February, 2021
Tuning-free Prompting

PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains24 February, 2021
LM+Prompt Tuning

Calibrate Before Use: Improving Few-Shot Performance of Language Models19 February, 2021
Tuning-free Prompting

Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm15 February, 2021
Tuning-free Prompting

What Makes Good In-Context Examples for GPT-3?17 January, 2021
Tuning-free Prompting

Prefix-Tuning: Optimizing Continuous Prompts for Generation1 January, 2021
Fixed-LM Prompt Tuning

WARP: Word-level Adversarial ReProgramming1 January, 2021
Fixed-LM Prompt Tuning

Making Pre-trained Language Models Better Few-shot Learners31 December, 2020
Fixed-prompt LM Tuning

Few-Shot Text Generation with Pattern-Exploiting Training22 December, 2020
Fixed-prompt LM Tuning

CTRLsum: Towards Generic Controllable Text Summarization8 December, 2020
Fixed-prompt LM Tuning

AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts29 October, 2020
Tuning-free Prompting

Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification26 October, 2020
Fixed-prompt LM Tuning

X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models13 October, 2020
Tuning-free Prompting

A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks7 October, 2020
Fixed-LM Prompt Tuning

RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models24 September, 2020
Tuning-free Prompting

It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners15 September, 2020
Fixed-prompt LM Tuning

Designing Templates for Eliciting Commonsense Knowledge from Pretrained Sequence-to-Sequence Models8 September, 2020
Fixed-prompt LM Tuning

Language Models are Few-Shot Learners28 May, 2020
Tuning-free Prompting

Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks22 May, 2020
LM+Prompt Tuning

UNIFIEDQA: Crossing Format Boundaries with a Single QA System2 May, 2020
Fixed-prompt LM Tuning

How Context Affects Language Models' Factual Predictions10 March, 2020
Tuning-free Prompting

Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference21 January, 2020
Fixed-prompt LM Tuning

Zero-shot Text Classification With Generative Language Models10 December, 2019
Fixed-prompt LM Tuning

How Can We Know What Language Models Know?28 November, 2019
Tuning-free Prompting

Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly8 November, 2019
Tuning-free Prompting

Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer23 October, 2019
Fixed-prompt LM Tuning

CTRL: A Conditional Transformer Language Model for Controllable Generation11 September, 2019
Fixed-prompt LM Tuning

Language Models as Knowledge Bases?3 September, 2019
Tuning-free Prompting

Commonsense Knowledge Mining from Pretrained Models2 September, 2019
Tuning-free Prompting

Universal Adversarial Triggers for Attacking and Analyzing NLP20 August, 2019
Tuning-free Prompting

What BERT is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models31 July, 2019
Tuning-free Prompting

Rare Words: A Major Problem for Contextualized Embeddings and How to Fix it by Attentive Mimicking14 April, 2019
Tuning-free Prompting

Language Models are Unsupervised Multitask Learners14 February, 2019
Tuning-free Prompting

\[1\]. ![](https://img.shields.io/badge/2020_02_12-blue) 
 [A Simple Method for Commonsense Reasoning]() 
Tuning-free Prompting


 ![](https://img.shields.io/badge/2020_02_12-blue) 
\[1\]. [A Simple Method for Commonsense Reasoning]() 
Tuning-free Prompting


#### \[1\] [A Simple Method for Commonsense Reasoning]()      ```2020-02-12``` 

*Timo Schick,Hinrich Schütze*


